# Scalable Scholarly Knowledge Graph

> Mining Research Papers & Citation Networks at Scale

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Spark 3.5](https://img.shields.io/badge/spark-3.5-orange.svg)](https://spark.apache.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A unified scholarly knowledge graph platform that ingests data from arXiv, PubMed, and OpenAlex, models it as a citation network, runs scalable analytics, and exposes insights via a polished web dashboard.

**Team**: Aarya Shah, Aryan Donde  
**Course**: CS-GY 6513 Big Data (NYU Tandon)

---

## ğŸ¯ Project Overview

This project demonstrates end-to-end big data engineering by building a scholarly knowledge graph from multiple open data sources. We:

1. **Ingest** millions of research papers from arXiv, PubMed, and OpenAlex APIs
2. **Model** data as a unified knowledge graph (papers, authors, venues, citations)
3. **Analyze** at scale using Spark, GraphFrames, and MLlib
4. **Serve** results via Elasticsearch and a FastAPI backend
5. **Visualize** insights in a production-quality Next.js dashboard

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES (APIs)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     arXiv       â”‚     PubMed      â”‚            OpenAlex                 â”‚
â”‚  (CS/Physics)   â”‚   (Biomedical)  â”‚     (Works + Citations)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                       â”‚
         â–¼                 â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION LAYER (Python)                             â”‚
â”‚  â€¢ Rate limiting & exponential backoff                                  â”‚
â”‚  â€¢ Checkpointing & resumability                                         â”‚
â”‚  â€¢ NDJSON output to HDFS raw zone                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER (HDFS)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RAW ZONE                      â”‚  PROCESSED ZONE                        â”‚
â”‚  /data/raw/{source}/{date}/    â”‚  /data/processed/{table}/year={YYYY}/  â”‚
â”‚  â€¢ NDJSON files                â”‚  â€¢ Parquet files (partitioned)         â”‚
â”‚  â€¢ Immutable                   â”‚  â€¢ Columnar, compressed                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROCESSING LAYER (Spark)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ETL Jobs              â”‚  Analytics Jobs                                â”‚
â”‚  â€¢ Schema normalizationâ”‚  â€¢ Topic modeling (TF-IDF + LDA)              â”‚
â”‚  â€¢ Deduplication       â”‚  â€¢ PageRank (GraphFrames)                     â”‚
â”‚  â€¢ ID resolution       â”‚  â€¢ Community detection                        â”‚
â”‚  â€¢ Citation linking    â”‚  â€¢ Trend analysis                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVING LAYER                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Elasticsearch                  â”‚  FastAPI Backend                      â”‚
â”‚  â€¢ Full-text search             â”‚  â€¢ REST API                           â”‚
â”‚  â€¢ Faceted filtering            â”‚  â€¢ Parquet aggregates                 â”‚
â”‚  â€¢ Influence metrics            â”‚  â€¢ Graph queries                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER (Next.js)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Dashboard â”‚ Search â”‚ Topics â”‚ Rankings â”‚ Graph Explorer â”‚ Pipeline     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Why This is Big Data

This architecture is designed for **scale beyond a single machine**:

| Component | Why It's Big Data |
|-----------|-------------------|
| **HDFS** | Distributed file system with replication; scales horizontally |
| **Parquet** | Columnar format with predicate pushdown; enables partition pruning |
| **Spark** | Distributed compute engine; processes data across worker nodes |
| **GraphFrames** | Distributed graph processing; PageRank on billion-edge graphs |
| **Elasticsearch** | Distributed search with sharding; handles millions of documents |

### Partitioning Strategy

- **Raw data**: Partitioned by `source/ingest_date` for incremental ingestion
- **Processed data**: Partitioned by `year` for time-range queries
- **Analytics outputs**: Pre-aggregated for dashboard performance

### Scalability Proof Points

1. **Horizontal scaling**: Add Spark workers to increase processing capacity
2. **Partition pruning**: Year-based partitioning enables efficient time filtering
3. **Distributed joins**: Citation graph built via Spark broadcast + shuffle joins
4. **Search sharding**: Elasticsearch auto-distributes across nodes

---

## ğŸ“ Repository Structure

```
/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                    # Next.js frontend
â”‚   â”‚   â”œâ”€â”€ app/                # App router pages
â”‚   â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â””â”€â”€ lib/                # Utilities
â”‚   â””â”€â”€ api/                    # FastAPI backend
â”‚       â”œâ”€â”€ routers/            # API routes
â”‚       â”œâ”€â”€ services/           # Business logic
â”‚       â””â”€â”€ models/             # Pydantic models
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ ingest/                 # Python ingestion jobs
â”‚   â”‚   â”œâ”€â”€ sources/            # API clients (arXiv, PubMed, OpenAlex)
â”‚   â”‚   â””â”€â”€ utils/              # Shared utilities
â”‚   â””â”€â”€ spark/                  # PySpark jobs
â”‚       â”œâ”€â”€ etl/                # ETL jobs
â”‚       â””â”€â”€ analytics/          # Analytics jobs
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml      # Full stack orchestration
â”‚   â”œâ”€â”€ spark/                  # Spark configs
â”‚   â”œâ”€â”€ hdfs/                   # HDFS configs
â”‚   â””â”€â”€ elasticsearch/          # ES configs
â”œâ”€â”€ data/                       # Local data (gitignored)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ REPORT.md               # Business report
â”‚   â”œâ”€â”€ SLIDES_OUTLINE.md       # Presentation outline
â”‚   â””â”€â”€ ARCHITECTURE.md         # Detailed architecture
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ demo.yaml               # Small demo config
â”‚   â””â”€â”€ full.yaml               # Full ingestion config
â”œâ”€â”€ Makefile                    # Build & run commands
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose (v2.0+)
- 16GB RAM recommended (8GB minimum for demo mode)
- 20GB disk space

### One-Command Demo

```bash
# Clone and enter the repository
git clone <repo-url>
cd scholarly-knowledge-graph

# Start everything and run the demo pipeline
make demo

# Open the dashboard
open http://localhost:3000
```

### Step-by-Step

```bash
# 1. Start infrastructure
make up

# 2. Run ingestion (demo mode: ~1000 papers)
make ingest CONFIG=demo

# 3. Run Spark ETL
make etl

# 4. Run analytics (topics, PageRank, communities)
make analytics

# 5. Index to Elasticsearch
make index

# 6. Open dashboard
open http://localhost:3000
```

### Useful Commands

```bash
make up          # Start all containers
make down        # Stop all containers
make logs        # View container logs
make spark-shell # Open interactive Spark shell
make status      # Check pipeline status
make clean       # Remove all data and containers
```

---

## ğŸŒ Deployment

### Local Development

Uses Docker Compose with pseudo-distributed HDFS:

```bash
make up
```

### Single Cloud VM (Production)

Deploy to an Ubuntu VM (recommended: 4+ vCPUs, 16GB+ RAM):

```bash
# SSH into your VM
ssh user@your-vm-ip

# Clone repository
git clone <repo-url>
cd scholarly-knowledge-graph

# Install Docker
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# Configure for production
cp configs/production.yaml configs/active.yaml
# Edit configs/active.yaml with your settings

# Start the stack
make up-prod

# Run the full pipeline
make demo CONFIG=production
```

The app will be accessible at `http://your-vm-ip:3000`

For HTTPS, add a reverse proxy (nginx/caddy) in front.

---

## ğŸ“Š Data Sources

### arXiv API
- **Endpoint**: `http://export.arxiv.org/api/query`
- **Data**: CS, Physics, Math papers with metadata
- **Rate limit**: 1 request/3 seconds (respected)
- **Fields**: id, title, abstract, authors, categories, dates

### PubMed E-utilities
- **Endpoint**: `https://eutils.ncbi.nlm.nih.gov/entrez/eutils/`
- **Data**: Biomedical literature
- **Rate limit**: 3 requests/second (10 with API key)
- **Fields**: PMID, title, abstract, authors, MeSH terms, journal

### OpenAlex
- **Endpoint**: `https://api.openalex.org/works`
- **Data**: 250M+ works with citation links
- **Rate limit**: 100K requests/day (polite pool)
- **Fields**: OpenAlex ID, DOI, citations, concepts, authors

---

## ğŸ§ª Analytics

### Topic Modeling (LDA)
- TF-IDF vectorization of abstracts
- Latent Dirichlet Allocation with 20-50 topics
- Topic coherence scoring for quality

### Citation Analysis
- **PageRank**: Influence score considering citation network structure
- **Citation count**: Raw popularity metric
- **Comparison**: PageRank surfaces "hidden gems" with fewer citations

### Community Detection
- Label Propagation on citation graph
- Identifies research clusters/subfields
- Used for graph coloring in UI

### Trend Analysis
- Topic share over time (rolling averages)
- Emerging topic detection (growth rate > threshold)
- Velocity metrics for "hot" research areas

---

## ğŸ–¥ï¸ UI Pages

| Page | Description |
|------|-------------|
| **Home** | KPI tiles, emerging topics, system health |
| **Search** | Elasticsearch-powered search with filters |
| **Topics** | Topic trends over time, drill-down to papers |
| **Rankings** | Papers/authors by PageRank vs citations |
| **Graph** | Interactive citation network explorer |
| **Pipeline** | Ingestion stats, data quality metrics |

---

## ğŸ“– Documentation

- [Architecture Deep Dive](docs/ARCHITECTURE.md)
- [Business Report](docs/REPORT.md)
- [Presentation Slides](docs/SLIDES_OUTLINE.md)

---

## ğŸ‘¥ Team

| Name | Role | Contributions |
|------|------|---------------|
| Aarya Shah | Data Engineering Lead | Ingestion, Spark ETL, HDFS |
| Aryan Donde | Analytics & Frontend Lead | GraphFrames, LDA, Next.js |

---

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- arXiv for open access to research metadata
- NCBI/NLM for PubMed E-utilities
- OpenAlex for the comprehensive scholarly graph
- NYU Tandon CS-GY 6513 course staff

